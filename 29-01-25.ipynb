{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c0b80e4-4f8b-4946-957f-82fad7a1aeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_trump = \"Mr. Trump became president after winning the political election. Though he lost the support of some republican friends, Trump is friends with President Putin.\"\n",
    "doc_election = \"President Trump says Putin had no political interference in the election outcome. He says it was a witchhunt by political parties. He claimed President Putin is a friend who had nothing to do with the election.\"\n",
    "doc_putin = \"Post election, Vladimir Putin became President of Russia. President Putin had served as the Prime Minister in his political career.\"\n",
    "documents = [doc_trump, doc_election, doc_putin]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "449da3a7-dbdd-4def-b00c-680e56324b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>after</th>\n",
       "      <th>as</th>\n",
       "      <th>became</th>\n",
       "      <th>by</th>\n",
       "      <th>career</th>\n",
       "      <th>claimed</th>\n",
       "      <th>do</th>\n",
       "      <th>election</th>\n",
       "      <th>friend</th>\n",
       "      <th>friends</th>\n",
       "      <th>...</th>\n",
       "      <th>the</th>\n",
       "      <th>though</th>\n",
       "      <th>to</th>\n",
       "      <th>trump</th>\n",
       "      <th>vladimir</th>\n",
       "      <th>was</th>\n",
       "      <th>who</th>\n",
       "      <th>winning</th>\n",
       "      <th>witchhunt</th>\n",
       "      <th>with</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc_trump</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_election</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_putin</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              after  as  became  by  career  claimed  do  election  friend  \\\n",
       "doc_trump         1   0       1   0       0        0   0         1       0   \n",
       "doc_election      0   0       0   1       0        1   1         2       1   \n",
       "doc_putin         0   1       1   0       1        0   0         1       0   \n",
       "\n",
       "              friends  ...  the  though  to  trump  vladimir  was  who  \\\n",
       "doc_trump           2  ...    2       1   0      2         0    0    0   \n",
       "doc_election        0  ...    2       0   1      1         0    1    1   \n",
       "doc_putin           0  ...    1       0   0      0         1    0    0   \n",
       "\n",
       "              winning  witchhunt  with  \n",
       "doc_trump           1          0     1  \n",
       "doc_election        0          1     1  \n",
       "doc_putin           0          0     0  \n",
       "\n",
       "[3 rows x 46 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scikit Learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Create the Document Term Matrix\n",
    "count_vect = CountVectorizer(stop_words='english')\n",
    "count_vect = CountVectorizer()\n",
    "sparse_matrix = count_vect.fit_transform(documents)\n",
    "\n",
    "# OPTIONAL: Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.\n",
    "doc_term_matrix = sparse_matrix.todense()\n",
    "df = pd.DataFrame(doc_term_matrix,\n",
    "                  columns=count_vect.get_feature_names_out(),\n",
    "                  index=['doc_trump', 'doc_election', 'doc_putin'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f7a1730f-9f5f-4224-814e-50cb405593dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.5        0.4330127 ]\n",
      " [0.5        1.         0.49074773]\n",
      " [0.4330127  0.49074773 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(cosine_similarity(df,df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0a492ba8-ddd4-49d6-b67e-1d6fa8df0cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnB=  {1, 2}\n",
      "AuB=  {1, 2, 3, 4, 5, 6, 8, 9}\n",
      "J(A,B)=  0.25\n"
     ]
    }
   ],
   "source": [
    "A={1,2,3,4,6}\n",
    "B={1,2,5,8,9}\n",
    "C=A.intersection(B)\n",
    "D=A.union(B)\n",
    "print('AnB= ',C)\n",
    "print('AuB= ',D)\n",
    "print('J(A,B)= ',float(len(C))/float(len(D)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0ffa0f2b-4be5-4c34-b54d-b844d45b02bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Similarity: 0.375\n"
     ]
    }
   ],
   "source": [
    "def jaccard_similarity(set1,set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union=len(set1.union(set2))\n",
    "    return intersection/union\n",
    "set_a={\"Language\",\"for\",\"Computer\",\"NLP\",\"Science\"}\n",
    "set_b={\"NLP\",\"for\",\"Language\",\"Data\",\"ML\",\"AI\"}\n",
    "similarity=jaccard_similarity(set_a,set_b)\n",
    "print(\"Jaccard Similarity:\",similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9302d9d-39e0-4ff6-b01f-92c340e4e485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f492206b-ed4a-4fb2-9f4a-e54e0aa4170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = [\n",
    "    \"You can return an item within 7 days of purchase.\",\n",
    "    \"Our return policy allows you to return items that are unopened and in their original condition.\",\n",
    "    \"We offer free shipping on orders over $50.\",\n",
    "    \"To track your order, you can visit the 'Order Tracking' page and enter your order number.\",\n",
    "    \"Our customer support team is available from 9 AM to 6 PM, Monday through Friday.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5d10a08e-e0d2-457c-80d4-4bba5656f75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input=\"How can I track my order?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "950d8c80-d9b2-4a1a-beea-61486ab50641",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=TfidfVectorizer(stop_words='english')\n",
    "all_texts=responses+[user_input]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "678a0d9a-4b92-40b0-91fe-dfd64fc8cf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix=vectorizer.fit_transform(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5c0a5c8c-4790-4ac1-b3e3-26f7e8a24346",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_vector=tfidf_matrix[-1]\n",
    "response_vectors=tfidf_matrix[:-1]\n",
    "cosine_similarities=cosine_similarity(user_vector,response_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1480fb9f-cbfe-4d5c-90ea-bf9afcf114b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar_idx=np.argmax(cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dbc220a0-cb4d-446a-bc6d-ea5832493126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: How can I track my order?\n",
      "Most relevant response: To track your order, you can visit the 'Order Tracking' page and enter your order number.\n"
     ]
    }
   ],
   "source": [
    "print(f\"User Query: {user_input}\")\n",
    "print(f\"Most relevant response: {responses[most_similar_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3a79aea0-6c4c-49a8-9e46-a0ce35c94309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import random\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "abbcf003-2b9d-44a2-85a9-31c3bc7a7794",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a2923d5b-9e5c-4b43-b74f-0b3bf5e11fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('popular',quiet=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0a678fd5-8007-4a6a-adcb-6aef72a2c361",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\GUNNALA\n",
      "[nltk_data]     VARSHITHA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "57cb36c8-fa65-46d1-8789-d5544339bc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\GUNNALA\n",
      "[nltk_data]     VARSHITHA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "71929c16-0ff9-4765-bd10-8c11f9a7627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('input.txt','r',errors='ignore')\n",
    "raw=f.read()\n",
    "raw=raw.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b00d38bf-d8b9-4e18-9908-9fc505936ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\GUNNALA\n",
      "[nltk_data]     VARSHITHA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1b066d0c-8f62-4556-96dc-245ea9007035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "sent_tokens = nltk.sent_tokenize(raw)  # Tokenize into sentences\n",
    "word_tokens = nltk.word_tokenize(raw)  # Tokenize into words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "67a367da-941c-4e9d-b363-6cb5ed68a6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'an', 'example', 'sentence', 'with', 'some', 'punctuation']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "# Initialize the Lemmatizer\n",
    "lemmer = nltk.WordNetLemmatizer()\n",
    "\n",
    "# Function to Lemmatize tokens\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Create a dictionary to remove punctuation\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "# Function to normalize text (lowercase, remove punctuation, and lemmatize)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "\n",
    "# Example usage:\n",
    "raw_text = \"This is an example sentence, with some punctuation!\"\n",
    "normalized_tokens = LemNormalize(raw_text)\n",
    "print(normalized_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ef2b68-136d-423f-885f-e6351e9fab01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
